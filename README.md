# Low-Quality Pill Images Classification

Intro2AI project - Advanced image classification using Masked Autoencoder with ResNet+FPN+CBAM architecture

## ğŸ“‹ MÃ´ táº£ dá»± Ã¡n

Dá»± Ã¡n phÃ¢n loáº¡i áº£nh viÃªn thuá»‘c cháº¥t lÆ°á»£ng tháº¥p sá»­ dá»¥ng kiáº¿n trÃºc deep learning tiÃªn tiáº¿n káº¿t há»£p:
- **ResNet Backbone 6 layers**: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng Ä‘a táº§ng
- **Feature Pyramid Network (FPN)**: Tá»•ng há»£p thÃ´ng tin Ä‘a tá»‰ lá»‡
- **CBAM (Convolutional Block Attention Module)**: CÆ¡ cháº¿ attention kÃªnh vÃ  khÃ´ng gian
- **Masked Autoencoder**: Há»c biá»ƒu diá»…n robust thÃ´ng qua reconstruction

## ğŸ—ï¸ Kiáº¿n trÃºc Model

### Tá»•ng quan luá»“ng dá»¯ liá»‡u

```
Input (224Ã—224Ã—3)
    â†“
ResNetBackbone6Layers
    â”œâ”€â†’ [c1, c2, c3, c4] â†’ FPN â†’ SharedFC â†’ MainHead â†’ Main Classification
    â””â”€â†’ c6 (2048ch) â†’ CBAM â†’ AuxiliaryHead â†’ Aux Classification
                        â†“
                   AttentionMap
                        â†“
                 GridMaskSelector
                        â†“
              Masked Input â†’ Backbone+FPN â†’ SharedFC â†’ ReconstructionHead
```

### Chi tiáº¿t cÃ¡c thÃ nh pháº§n

#### 1. ResNetBackbone6Layers (káº¿ thá»«a CNN/ResNet18)
- **Input**: 224Ã—224Ã—3
- **Stem**: Conv7Ã—7 s2 â†’ BN/ReLU â†’ AdaptivePool â†’ 96Ã—96Ã—64
- **6 Layers**:
  - Layer 1: 96Ã—96Ã—64 (cho FPN)
  - Layer 2: 48Ã—48Ã—128 (cho FPN)
  - Layer 3: 24Ã—24Ã—256 (cho FPN)
  - Layer 4: 12Ã—12Ã—512 (cho FPN)
  - Layer 5: 6Ã—6Ã—1024 (trung gian)
  - Layer 6: 3Ã—3Ã—2048 (cho CBAM)
- **Output**: `[c1, c2, c3, c4], c6`

#### 2. Feature Pyramid Network (FPN)
- **Input**: [c1:64ch, c2:128ch, c3:256ch, c4:512ch]
- **Lateral Conv**: Chuáº©n hÃ³a vá» 256 kÃªnh
- **Top-down pathway**: Tá»•ng há»£p thÃ´ng tin tá»« thÃ´ â†’ tinh
- **Output**: [p2:96Ã—96, p3:48Ã—48, p4:24Ã—24, p5:12Ã—12] Ã— 256ch

#### 3. CBAM Attention
- **Input**: c6 (3Ã—3Ã—2048)
- **Channel Attention**: Avg/Max pooling â†’ FC â†’ Sigmoid
- **Spatial Attention**: Channel-wise avg/max â†’ Conv â†’ Sigmoid
- **Output**: Enhanced feature + Attention map

#### 4. GridMaskSelector
- **Input**: Attention map + Original image
- **Logic**: 
  - Chia attention map thÃ nh lÆ°á»›i 3Ã—3
  - TÃ¬m vÃ¹ng 2Ã—2 cÃ³ tá»•ng attention cao nháº¥t
  - Táº¡o mask che 4/9 áº£nh
- **Output**: Masked image

#### 5. SharedFC (Encoder)
- **Input**: FPN feature lá»›n nháº¥t (96Ã—96Ã—256)
- **CNN Encoder**: 
  - 96Ã—96 â†’ 48Ã—48 â†’ 24Ã—24 â†’ 12Ã—12 â†’ 6Ã—6 (512ch)
- **FC Block**: 
  - Flatten â†’ Linear(18432â†’1024) â†’ Linear(1024â†’512)
- **Output**: Latent vector 512-dim

#### 6. Classification Heads
- **MainHead**: Latent 512 â†’ FC â†’ 15 classes (tá»« clean image)
- **AuxiliaryHead**: CBAM feature 2048 â†’ AvgPool â†’ FC â†’ 15 classes

#### 7. ReconstructionHead
- **Input**: Latent 512-dim (tá»« masked image)
- **FC Expand**: 512 â†’ 25088 (512Ã—7Ã—7)
- **Decoder**: 5 ConvTranspose layers
  - 7Ã—7 â†’ 14Ã—14 â†’ 28Ã—28 â†’ 56Ã—56 â†’ 112Ã—112 â†’ 224Ã—224
- **Output**: Reconstructed image 224Ã—224Ã—3

## ğŸ“Š Loss Function

```python
Total Loss = Main Loss + Î»_aux Ã— Aux Loss + Î»_rec Ã— Rec Loss
```

- **Main Loss**: CrossEntropyLoss (classification chÃ­nh)
- **Aux Loss**: CrossEntropyLoss (auxiliary supervision, Î»=0.4)
- **Rec Loss**: Masked MSE Loss (reconstruction, Î»=0.5)

## ğŸ”§ Hyperparameters

```python
BATCH_SIZE = 8
NUM_CLASSES = 15
EPOCHS = 10-30 (khuyáº¿n nghá»‹ 20-30 cho ~47M params)
LEARNING_RATE = 1e-4
OPTIMIZER = Adam
LAMBDA_AUX = 0.4
LAMBDA_REC = 0.5
```

## ğŸ› ï¸ Requirements

```
torch >= 1.10.0
torchvision >= 0.11.0
pandas
PIL
tqdm
```

## ğŸ“ Notes

- **Checkpoint tá»± Ä‘á»™ng**: LÆ°u má»—i N epochs vÃ o `model_checkpoints/`
- **Loss weights**: Lambda cÃ³ thá»ƒ Ä‘iá»u chá»‰nh tÃ¹y dataset
- **Grid size**: Hiá»‡n táº¡i 3Ã—3, cÃ³ thá»ƒ tÃ¹y chá»‰nh trong `GridMaskSelector`
- **Image normalization**: ImageNet statistics [0.485, 0.456, 0.406] / [0.229, 0.224, 0.225]

## ğŸ“§ Contact

Intro2AI Project - Nguyá»…n CÃ´ng HÃ¹ng - HUST IT-E10 03 K69
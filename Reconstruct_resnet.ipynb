{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3XlHpRaMAoAs"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jO0eGrVRN6kF"
   },
   "outputs": [],
   "source": [
    "train_data_path = r\"F:\\PAST\\HUST\\IT-E10\\Introduction to AI\\Intro2AI\\train\"\n",
    "train_label_path = r\"F:\\PAST\\HUST\\IT-E10\\Introduction to AI\\Intro2AI\\train_label.csv\"\n",
    "train_labels = pd.read_csv(train_label_path)\n",
    "train_dict = train_labels.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DytOtNlzCxGZ"
   },
   "outputs": [],
   "source": [
    "class TrainImageDataset(Dataset):\n",
    "    def __init__(self, data_dict, input_path, transform=None):\n",
    "        self.data_dict = data_dict\n",
    "        self.input_path = input_path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data_dict[idx]['image_name']\n",
    "        label = self.data_dict[idx]['label']\n",
    "\n",
    "        img_path = os.path.join(self.input_path, img_name)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Dpk-aICfDQPv"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tKqw7lkSDS3i"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_dataset = TrainImageDataset(data_dict=train_dict, input_path=train_data_path, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Gd110z4AbXnm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")  \n",
    "NUM_CLASSES = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ARSoVjF0xDrv"
   },
   "outputs": [],
   "source": [
    "# !!! Do not change anything of this cell !!!\n",
    "from torch import Tensor\n",
    "from typing import Type\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        expansion: int = 1,\n",
    "        downsample: nn.Module = None,\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.expansion = expansion\n",
    "        self.downsample = downsample\n",
    "        self.conv1_layer = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2_layer = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels * self.expansion,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1_layer(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2_layer(out)\n",
    "        out = self.batch_norm2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CNN(nn.Module): #ResNet18\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[BasicBlock],\n",
    "        img_channels: int = 3,\n",
    "        num_classes: int = 10,\n",
    "    ) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        layers = [2, 2, 2, 2]\n",
    "        self.expansion = 1\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv_layer = nn.Conv2d(\n",
    "            in_channels=img_channels,\n",
    "            out_channels=self.in_channels,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.batch_norm = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool_layer = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer_1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer_2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer_3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer_4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool_layer = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc_layer = nn.Linear(512 * self.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self, block: Type[BasicBlock], out_channels: int, blocks: int, stride: int = 1\n",
    "    ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    out_channels * self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(self.in_channels, out_channels, stride, self.expansion, downsample)\n",
    "        )\n",
    "        self.in_channels = out_channels * self.expansion\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(self.in_channels, out_channels, expansion=self.expansion)\n",
    "            )\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool_layer(x)\n",
    "\n",
    "        c2 = self.layer_1(x)\n",
    "        c3 = self.layer_2(c2)\n",
    "        c4 = self.layer_3(c3)\n",
    "        c5 = self.layer_4(c4)\n",
    "\n",
    "        x = self.avgpool_layer(c5)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        # Channel Attention Components\n",
    "        self.ca_avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.ca_max = nn.AdaptiveMaxPool2d(1)\n",
    "        self.ca_fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.ca_relu = nn.ReLU()\n",
    "        self.ca_fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Spatial Attention Components\n",
    "        self.sa_conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Channel Attention\n",
    "        avg_out = self.ca_fc2(self.ca_relu(self.ca_fc1(self.ca_avg(x))))\n",
    "        max_out = self.ca_fc2(self.ca_relu(self.ca_fc1(self.ca_max(x))))\n",
    "        x = x * self.sigmoid(avg_out + max_out)\n",
    "        # Spatial Attention\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        sa_map = self.sigmoid(self.sa_conv(torch.cat([avg_out, max_out], dim=1)))\n",
    "        return x * sa_map, sa_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryHead(nn.Module):\n",
    "    def __init__(self, in_channels=2048, num_classes=15):\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(in_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avgpool(x).flatten(1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMaskSelector(nn.Module):\n",
    "    \"\"\"Responsible for logic: 3x3 Grid -> Select Best 2x2 Square -> Create Mask\"\"\"\n",
    "    def __init__(self, grid_size=3):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size \n",
    "\n",
    "    def forward(self, attention_map, input_img):\n",
    "        B, _, H, W = input_img.shape\n",
    "        # 1. Downsample to grid\n",
    "        grid_att = F.adaptive_avg_pool2d(attention_map, (self.grid_size, self.grid_size))\n",
    "        \n",
    "        # 2. Find best 2x2 window\n",
    "        with torch.no_grad():\n",
    "            kernel = torch.ones((1, 1, 2, 2)).to(attention_map.device)\n",
    "            sum_att = F.conv2d(grid_att, kernel, stride=1).view(B, -1)\n",
    "            best_idx = torch.argmax(sum_att, dim=1)\n",
    "\n",
    "        # 3. Create Mask\n",
    "        mask = torch.zeros((B, 1, H, W)).to(input_img.device)\n",
    "        h_step, w_step = H // self.grid_size, W // self.grid_size\n",
    "        \n",
    "        for b in range(B):\n",
    "            idx = best_idx[b].item()\n",
    "            r, c = (idx // 2), (idx % 2)\n",
    "            mask[b, :, r*h_step:(r+2)*h_step, c*w_step:(c+2)*w_step] = 1.0\n",
    "\n",
    "        return input_img * (1 - mask), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBackbone6Layers(CNN):\n",
    "    \"\"\"6-layer backbone built on top of CNN while preserving the original spatial/channel logic.\"\"\"\n",
    "    def __init__(self, block, img_channels=3):\n",
    "        # Reuse CNN stem and first 4 layers\n",
    "        super().__init__(block=block, img_channels=img_channels, num_classes=1)  # num_classes unused here\n",
    "        \n",
    "        # Force spatial size to 96x96 after the stem (replace prior maxpool effect)\n",
    "        self.force_spatial_96 = nn.AdaptiveAvgPool2d((96, 96))\n",
    "        \n",
    "        self.in_channels = 512  # state after layer_4 in the parent\n",
    "        self.layer_5 = self._make_layer(block, 1024, blocks=2, stride=2)  \n",
    "        self.layer_6 = self._make_layer(block, 2048, blocks=2, stride=2)  \n",
    "        \n",
    "        self._init_new_layers()\n",
    "        \n",
    "    def _init_new_layers(self):\n",
    "        # Initialize only the newly added layers to match prior custom init\n",
    "        for m in [self.layer_5, self.layer_6]:\n",
    "            for sub in m.modules():\n",
    "                if isinstance(sub, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(sub.weight, mode='fan_out', nonlinearity='relu')\n",
    "                elif isinstance(sub, nn.BatchNorm2d):\n",
    "                    nn.init.constant_(sub.weight, 1)\n",
    "                    nn.init.constant_(sub.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Stem: conv7x7 stride2 -> BN/ReLU -> force to 96x96\n",
    "        x = self.conv_layer(x)            # [B, 64, 112, 112]\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.force_spatial_96(x)     # [B, 64, 96, 96]\n",
    "        \n",
    "        # Shared CNN layers 1-4\n",
    "        c1 = self.layer_1(x)              # [B, 64, 96, 96]\n",
    "        c2 = self.layer_2(c1)            # [B, 128, 48, 48]\n",
    "        c3 = self.layer_3(c2)            # [B, 256, 24, 24]\n",
    "        c4 = self.layer_4(c3)            # [B, 512, 12, 12]\n",
    "        \n",
    "        # Extra layers 5-6\n",
    "        c5 = self.layer_5(c4)            # [B, 1024, 6, 6]\n",
    "        c6 = self.layer_6(c5)            # [B, 2048, 3, 3]\n",
    "        \n",
    "        return [c1, c2, c3, c4], c6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, in_channels_list=[64, 128, 256, 512], out_channels=256):\n",
    "        super().__init__()\n",
    "        self.lateral = nn.ModuleList([nn.Conv2d(c, out_channels, 1) for c in in_channels_list])\n",
    "        self.smooth = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        c2, c3, c4, c5 = features\n",
    "        # Top-down pathway\n",
    "        p5 = self.lateral[3](c5)\n",
    "        p4 = self.lateral[2](c4) + F.interpolate(p5, scale_factor=2)\n",
    "        p3 = self.lateral[1](c3) + F.interpolate(p4, scale_factor=2)\n",
    "        p2 = self.lateral[0](c2) + F.interpolate(p3, scale_factor=2)\n",
    "        return [p2, p3, p4, p5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedFC(nn.Module):\n",
    "    def __init__(self, in_channels=256, latent_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder CNN: Nén FPN to nhất (96x96x256) xuống nhỏ hơn để vào FC\n",
    "        # Input: [B, 256, 96, 96]\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=3, stride=2, padding=1), # 96 -> 48\n",
    "            nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1), # 48 -> 24\n",
    "            nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1), # 24 -> 12\n",
    "            nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1), # 12 -> 6\n",
    "            nn.BatchNorm2d(512), nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # [512, 6, 6]\n",
    "        self.flatten_dim = 512 * 6 * 6 \n",
    "        \n",
    "        self.fc_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flatten_dim, 1024),\n",
    "            nn.BatchNorm1d(1024), nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim), nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder_cnn(x)\n",
    "        return self.fc_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainClassificationHead(nn.Module):\n",
    "    def __init__(self, latent_dim=512, num_classes=15):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionHead(nn.Module):\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super().__init__()\n",
    "        # Bung latent ra thành feature map để ConvTranspose\n",
    "        self.fc_expand = nn.Linear(latent_dim, 512 * 7 * 7) \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (512, 7, 7)),\n",
    "            # Upsample dần lên 224x224\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU(), # -> 14\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU(), # -> 28\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  nn.BatchNorm2d(64),  nn.ReLU(), # -> 56\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),   nn.BatchNorm2d(32),  nn.ReLU(), # -> 112\n",
    "            nn.ConvTranspose2d(32, 3, 4, 2, 1),    nn.Sigmoid()                    # -> 224\n",
    "        )\n",
    "\n",
    "    def forward(self, latent):\n",
    "        x = self.fc_expand(latent)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoderSystem(nn.Module):\n",
    "    def __init__(self, num_classes=15):\n",
    "        super().__init__()\n",
    "        self.backbone = ResNetBackbone6Layers(BasicBlock)\n",
    "        \n",
    "        # 2. FPN & CBAM & Selector\n",
    "        self.fpn = FPN(in_channels_list=[64, 128, 256, 512], out_channels=256)\n",
    "        self.cbam = CBAM(in_planes=2048)\n",
    "        self.mask_selector = GridMaskSelector(grid_size=3)\n",
    "        \n",
    "        self.aux_head = AuxiliaryHead(in_channels=2048, num_classes=num_classes)\n",
    "        \n",
    "        # Shared FC: Nhận từ FPN (256 ch)\n",
    "        self.shared_fc = SharedFC(in_channels=256, latent_dim=512)\n",
    "        \n",
    "        # Main Head & Rec Head: Nhận từ Shared FC (Latent 512)\n",
    "        self.main_head = MainClassificationHead(latent_dim=512, num_classes=num_classes)\n",
    "        self.rec_head = ReconstructionHead(latent_dim=512)\n",
    "\n",
    "    def forward(self, x, training_mode=True):\n",
    "        # PHASE 1: CLEAN PASS\n",
    "        fpn_feats, final_feat = self.backbone(x) # fpn_feats = [c1, c2, c3, c4], final_feat = c6\n",
    "        # Tính toán FPN\n",
    "        fpn_outs = self.fpn(fpn_feats) \n",
    "        # fpn_outs[0] là lớp to nhất (tương ứng c1: 96x96)\n",
    "        largest_fpn_feat = fpn_outs[0] \n",
    "        # CBAM (cho Aux Loss & Mask Selector)\n",
    "        feat_att, att_map = self.cbam(final_feat)\n",
    "        \n",
    "        # --- Output 1: Aux Logits (Từ CBAM) ---\n",
    "        aux_logits = self.aux_head(feat_att)\n",
    "        # --- Output 2: Main Logits (Từ FPN to nhất -> Shared FC) ---\n",
    "        clean_latent = self.shared_fc(largest_fpn_feat)\n",
    "        main_logits = self.main_head(clean_latent)\n",
    "        \n",
    "        \n",
    "        # PHASE 2: MASKED PASS\n",
    "        rec_img, mask = None, None\n",
    "        if training_mode:\n",
    "            masked_x, mask = self.mask_selector(att_map.detach(), x) #detach() tạo nhánh mới, không truyền gradient\n",
    "            \n",
    "            # Cần chạy cả FPN vì SharedFC giờ đây yêu cầu output của FPN\n",
    "            m_fpn_feats, _ = self.backbone(masked_x)\n",
    "            m_fpn_outs = self.fpn(m_fpn_feats)\n",
    "            \n",
    "            # Lấy feature map to nhất của ảnh bị mask\n",
    "            m_largest_fpn_feat = m_fpn_outs[0] \n",
    "            \n",
    "            masked_latent = self.shared_fc(m_largest_fpn_feat)\n",
    "            rec_img = self.rec_head(masked_latent)\n",
    "            \n",
    "        return {\n",
    "            \"aux_logits\": aux_logits,\n",
    "            \"main_logits\": main_logits,\n",
    "            \"rec_img\": rec_img,\n",
    "            \"mask\": mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, device, lr=1e-3, save_interval=None, checkpoint_dir=\"checkpoints\"):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.device = device\n",
    "        \n",
    "        # Trọng số cho các loss (Hyperparameters)\n",
    "        self.lambda_aux = 0.4  # Aux loss thường có trọng số nhỏ hơn main\n",
    "        self.lambda_rec = 0.5  # Reconstruction loss\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.cls_criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Checkpoint settings\n",
    "        self.save_interval = save_interval  # Lưu mỗi N epochs, None = không tự động lưu\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # Tạo thư mục checkpoint nếu chưa có\n",
    "        if self.save_interval is not None:\n",
    "            os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    def masked_mse_loss(self, pred, target, mask):\n",
    "        loss = F.mse_loss(pred, target, reduction='none')\n",
    "        loss = loss * mask\n",
    "        return loss.sum() / (mask.sum() + 1e-6)\n",
    "\n",
    "    def train_one_epoch(self, epoch_index):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Dùng tqdm để hiển thị progress bar đẹp hơn\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch_index}\")\n",
    "        \n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(self.device), labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.model(images, training_mode=True)\n",
    "            \n",
    "            # 1. Main Classification Loss\n",
    "            loss_main = self.cls_criterion(outputs[\"main_logits\"], labels)\n",
    "            \n",
    "            # 2. Auxiliary Classification Loss\n",
    "            loss_aux = self.cls_criterion(outputs[\"aux_logits\"], labels)\n",
    "            \n",
    "            # 3. Reconstruction Loss\n",
    "            loss_rec = self.masked_mse_loss(outputs[\"rec_img\"], images, outputs[\"mask\"])\n",
    "            \n",
    "            # Tổng hợp Loss\n",
    "            total_loss = loss_main + (self.lambda_aux * loss_aux) + (self.lambda_rec * loss_rec)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += total_loss.item()\n",
    "            \n",
    "            # Hiển thị chi tiết từng loss trên thanh tiến trình\n",
    "            pbar.set_postfix({\n",
    "                'T': f\"{total_loss.item():.2f}\", \n",
    "                'Main': f\"{loss_main.item():.2f}\",\n",
    "                'Aux': f\"{loss_aux.item():.2f}\",\n",
    "                'Rec': f\"{loss_rec.item():.2f}\"\n",
    "            })\n",
    "        \n",
    "        avg_loss = running_loss / len(self.train_loader)\n",
    "        \n",
    "        # Auto-save checkpoint nếu đến interval\n",
    "        if self.save_interval is not None and epoch_index % self.save_interval == 0:\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, f\"checkpoint_epoch_{epoch_index}.pth\")\n",
    "            self.save_checkpoint(checkpoint_path, epoch_index, avg_loss)\n",
    "            print(f\"\\n✓ Saved checkpoint at epoch {epoch_index} to {checkpoint_path}\")\n",
    "            \n",
    "        return avg_loss\n",
    "\n",
    "    def save_checkpoint(self, path, epoch, loss):\n",
    "        \"\"\"Lưu checkpoint với thông tin đầy đủ\"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, path)\n",
    "    \n",
    "    def load_checkpoint(self, path):\n",
    "        \"\"\"Load checkpoint để tiếp tục training\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch'], checkpoint['loss']\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Lưu chỉ model state (final model)\"\"\"\n",
    "        torch.save(self.model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706910250b9049118d5161331d8c2ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Avg Loss: 5.6071\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b0f474c6c64011af539301f5e34c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved checkpoint at epoch 2 to model_checkpoints\\checkpoint_epoch_2.pth\n",
      "Epoch 2/10 - Avg Loss: 4.9520\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f11d0972d040a0885c12ac13a4f713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 - Avg Loss: 4.4975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dc7973166f46b6b00caf6176eaf869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved checkpoint at epoch 4 to model_checkpoints\\checkpoint_epoch_4.pth\n",
      "Epoch 4/10 - Avg Loss: 4.1190\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f639354e734d2a94256545afd6a814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 - Avg Loss: 3.8097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f20abf7094e465fb9be43fa9397a2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_model = MaskedAutoencoderSystem(num_classes=NUM_CLASSES)\n",
    "\n",
    "# Khởi tạo Trainer với save_interval (ví dụ: lưu mỗi 2 epochs)\n",
    "trainer = Trainer(\n",
    "    system_model, \n",
    "    train_dataloader, \n",
    "    device, \n",
    "    lr=1e-4,\n",
    "    save_interval=2,           # Lưu checkpoint mỗi 2 epochs\n",
    "    checkpoint_dir=\"model_checkpoints\"  # Thư mục lưu checkpoint\n",
    ")\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    avg_loss = trainer.train_one_epoch(epoch)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} - Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "# Lưu model cuối cùng\n",
    "trainer.save(\"masked_resnet_aux_model_final.pth\")\n",
    "print(\"Training Complete!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MyProjectPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "008fa5c5eb7b4f4bab3ca043b9163151": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "07412aef72be4cd89f28ef26be3c2066": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a0c49f320c9456d95c505ee3c6b8495": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88994cae248a4ad1ba69e8af090cf24c",
      "placeholder": "​",
      "style": "IPY_MODEL_d59b9412ff7a4844b0838970c2d7eea5",
      "value": "100%"
     }
    },
    "775d73702d0c4037986a9f59707e9022": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9b06a7f2a11484eb91dc427196c0b28",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_008fa5c5eb7b4f4bab3ca043b9163151",
      "value": 20
     }
    },
    "88994cae248a4ad1ba69e8af090cf24c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2d7ebf3941c4fd6b82c08cf73fa4127": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a0c49f320c9456d95c505ee3c6b8495",
       "IPY_MODEL_775d73702d0c4037986a9f59707e9022",
       "IPY_MODEL_e332be71971943fab4de8e989b822810"
      ],
      "layout": "IPY_MODEL_c98eae38e2ea49bbbc17ac7cba5520a3"
     }
    },
    "c98eae38e2ea49bbbc17ac7cba5520a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cedb5e089445439ca60732cbe5a820db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d59b9412ff7a4844b0838970c2d7eea5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e332be71971943fab4de8e989b822810": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07412aef72be4cd89f28ef26be3c2066",
      "placeholder": "​",
      "style": "IPY_MODEL_cedb5e089445439ca60732cbe5a820db",
      "value": " 20/20 [02:10&lt;00:00,  6.53s/it]"
     }
    },
    "f9b06a7f2a11484eb91dc427196c0b28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
